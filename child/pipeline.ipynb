{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "file = 'Autism-Child-Data.txt'\n",
    "\n",
    "# read the csv\n",
    "data = pd.read_table(file, sep = ',', index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1_Score                      1\n",
      "A2_Score                      1\n",
      "A3_Score                      0\n",
      "A4_Score                      0\n",
      "A5_Score                      1\n",
      "A6_Score                      1\n",
      "A7_Score                      0\n",
      "A8_Score                      1\n",
      "A9_Score                      0\n",
      "A10_Score                     0\n",
      "age                           6\n",
      "gender                        m\n",
      "ethnicity                Others\n",
      "jundice                      no\n",
      "austim                       no\n",
      "contry_of_res            Jordan\n",
      "used_app_before              no\n",
      "result                        5\n",
      "age_desc           '4-11 years'\n",
      "relation                 Parent\n",
      "Class                        NO\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 292\n",
      "Individuals diagonised with ASD: 141\n",
      "Individuals not diagonised with ASD: 151\n",
      "Percentage of individuals diagonised with ASD: 48.29%\n"
     ]
    }
   ],
   "source": [
    "# Total number of records\n",
    "n_records = len(data.index)\n",
    "\n",
    "#Number of records where individual's with ASD\n",
    "n_asd_yes = len(data[data['Class'] == 'YES'])\n",
    "\n",
    "# Number of records where individual's with no ASD\n",
    "n_asd_no = len(data[data['Class'] == 'NO'])\n",
    "\n",
    "# Percentage of individuals whose are with ASD\n",
    "yes_percent = float(n_asd_yes) / n_records *100\n",
    "\n",
    "# Print the results\n",
    "print(\"Total number of records: {}\".format(n_records))\n",
    "print(\"Individuals diagonised with ASD: {}\".format(n_asd_yes))\n",
    "print(\"Individuals not diagonised with ASD: {}\".format(n_asd_no))\n",
    "print(\"Percentage of individuals diagonised with ASD: {:.2f}%\".format(yes_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd_data = pd.read_table(file, sep = ',', index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1_Score</th>\n",
       "      <th>A2_Score</th>\n",
       "      <th>A3_Score</th>\n",
       "      <th>A4_Score</th>\n",
       "      <th>A5_Score</th>\n",
       "      <th>A6_Score</th>\n",
       "      <th>A7_Score</th>\n",
       "      <th>A8_Score</th>\n",
       "      <th>A9_Score</th>\n",
       "      <th>A10_Score</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.633562</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.743151</td>\n",
       "      <td>0.551370</td>\n",
       "      <td>0.743151</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.606164</td>\n",
       "      <td>0.496575</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>6.239726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.482658</td>\n",
       "      <td>0.499682</td>\n",
       "      <td>0.437646</td>\n",
       "      <td>0.498208</td>\n",
       "      <td>0.437646</td>\n",
       "      <td>0.453454</td>\n",
       "      <td>0.489438</td>\n",
       "      <td>0.500847</td>\n",
       "      <td>0.500811</td>\n",
       "      <td>0.446761</td>\n",
       "      <td>2.284882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         A1_Score    A2_Score    A3_Score    A4_Score    A5_Score    A6_Score  \\\n",
       "count  292.000000  292.000000  292.000000  292.000000  292.000000  292.000000   \n",
       "mean     0.633562    0.534247    0.743151    0.551370    0.743151    0.712329   \n",
       "std      0.482658    0.499682    0.437646    0.498208    0.437646    0.453454   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "75%      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         A7_Score    A8_Score    A9_Score   A10_Score      result  \n",
       "count  292.000000  292.000000  292.000000  292.000000  292.000000  \n",
       "mean     0.606164    0.496575    0.493151    0.726027    6.239726  \n",
       "std      0.489438    0.500847    0.500811    0.446761    2.284882  \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000    0.000000    5.000000  \n",
       "50%      1.000000    0.000000    0.000000    1.000000    6.000000  \n",
       "75%      1.000000    1.000000    1.000000    1.000000    8.000000  \n",
       "max      1.000000    1.000000    1.000000    1.000000   10.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1_Score</th>\n",
       "      <th>A2_Score</th>\n",
       "      <th>A3_Score</th>\n",
       "      <th>A4_Score</th>\n",
       "      <th>A5_Score</th>\n",
       "      <th>A6_Score</th>\n",
       "      <th>A7_Score</th>\n",
       "      <th>A8_Score</th>\n",
       "      <th>A9_Score</th>\n",
       "      <th>A10_Score</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.633562</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.743151</td>\n",
       "      <td>0.551370</td>\n",
       "      <td>0.743151</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.606164</td>\n",
       "      <td>0.496575</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>6.239726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.482658</td>\n",
       "      <td>0.499682</td>\n",
       "      <td>0.437646</td>\n",
       "      <td>0.498208</td>\n",
       "      <td>0.437646</td>\n",
       "      <td>0.453454</td>\n",
       "      <td>0.489438</td>\n",
       "      <td>0.500847</td>\n",
       "      <td>0.500811</td>\n",
       "      <td>0.446761</td>\n",
       "      <td>2.284882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         A1_Score    A2_Score    A3_Score    A4_Score    A5_Score    A6_Score  \\\n",
       "count  292.000000  292.000000  292.000000  292.000000  292.000000  292.000000   \n",
       "mean     0.633562    0.534247    0.743151    0.551370    0.743151    0.712329   \n",
       "std      0.482658    0.499682    0.437646    0.498208    0.437646    0.453454   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "75%      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         A7_Score    A8_Score    A9_Score   A10_Score      result  \n",
       "count  292.000000  292.000000  292.000000  292.000000  292.000000  \n",
       "mean     0.606164    0.496575    0.493151    0.726027    6.239726  \n",
       "std      0.489438    0.500847    0.500811    0.446761    2.284882  \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000    0.000000    5.000000  \n",
       "50%      1.000000    0.000000    0.000000    1.000000    6.000000  \n",
       "75%      1.000000    1.000000    1.000000    1.000000    8.000000  \n",
       "max      1.000000    1.000000    1.000000    1.000000   10.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd_data.dropna(inplace=True)\n",
    "asd_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import fbeta_score, make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['gender', 'ethnicity', 'jundice', 'austim', 'contry_of_res', 'used_app_before', 'age_desc', 'relation']\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Convert target variable to numerical format\n",
    "label_encoder = LabelEncoder()\n",
    "data_encoded['Class'] = label_encoder.fit_transform(data_encoded['Class'])\n",
    "\n",
    "# Split data into features (X) and target (Y)\n",
    "X = data_encoded.drop(columns=['Class'])  # Features\n",
    "Y = data_encoded['Class']  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score',\n",
       "       'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age', 'result',\n",
       "       'gender_m', 'ethnicity_'South Asian'', 'ethnicity_Asian',\n",
       "       'ethnicity_Black', 'ethnicity_Hispanic', 'ethnicity_Latino',\n",
       "       'ethnicity_Others', 'ethnicity_Pasifika', 'ethnicity_Turkish',\n",
       "       'ethnicity_White-European', 'jundice_yes', 'austim_yes',\n",
       "       'contry_of_res_'Isle of Man'', 'contry_of_res_'New Zealand'',\n",
       "       'contry_of_res_'Saudi Arabia'', 'contry_of_res_'South Africa'',\n",
       "       'contry_of_res_'South Korea'', 'contry_of_res_'U.S. Outlying Islands'',\n",
       "       'contry_of_res_'United Arab Emirates'',\n",
       "       'contry_of_res_'United Kingdom'', 'contry_of_res_'United States'',\n",
       "       'contry_of_res_Afghanistan', 'contry_of_res_Argentina',\n",
       "       'contry_of_res_Armenia', 'contry_of_res_Australia',\n",
       "       'contry_of_res_Austria', 'contry_of_res_Bahrain',\n",
       "       'contry_of_res_Bangladesh', 'contry_of_res_Bhutan',\n",
       "       'contry_of_res_Brazil', 'contry_of_res_Bulgaria',\n",
       "       'contry_of_res_Canada', 'contry_of_res_China', 'contry_of_res_Egypt',\n",
       "       'contry_of_res_Europe', 'contry_of_res_Georgia',\n",
       "       'contry_of_res_Germany', 'contry_of_res_Ghana', 'contry_of_res_India',\n",
       "       'contry_of_res_Iraq', 'contry_of_res_Ireland', 'contry_of_res_Italy',\n",
       "       'contry_of_res_Japan', 'contry_of_res_Jordan', 'contry_of_res_Kuwait',\n",
       "       'contry_of_res_Latvia', 'contry_of_res_Lebanon', 'contry_of_res_Libya',\n",
       "       'contry_of_res_Malaysia', 'contry_of_res_Malta', 'contry_of_res_Mexico',\n",
       "       'contry_of_res_Nepal', 'contry_of_res_Netherlands',\n",
       "       'contry_of_res_Nigeria', 'contry_of_res_Oman', 'contry_of_res_Pakistan',\n",
       "       'contry_of_res_Philippines', 'contry_of_res_Qatar',\n",
       "       'contry_of_res_Romania', 'contry_of_res_Russia', 'contry_of_res_Sweden',\n",
       "       'contry_of_res_Syria', 'contry_of_res_Turkey', 'used_app_before_yes',\n",
       "       'relation_Parent', 'relation_Relative', 'relation_Self',\n",
       "       'relation_self'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      1\n",
       "      ..\n",
       "287    1\n",
       "288    0\n",
       "289    1\n",
       "290    1\n",
       "291    0\n",
       "Name: Class, Length: 292, dtype: int32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "# split the X and Y data into training and testing datasets\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "X_imputed = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Decision Tree\n",
      "Cross-validated Fbeta Score: 1.0\n",
      "Cross-validated ROC AUC Score: 1.0\n",
      "Fbeta Score on Test Set: 1.0\n",
      "[[25  0]\n",
      " [ 0 34]]\n",
      "===\n",
      "Evaluating Random Forest\n",
      "Cross-validated Fbeta Score: 0.9511589979331914\n",
      "Cross-validated ROC AUC Score: 0.9883333333333333\n",
      "Fbeta Score on Test Set: 1.0\n",
      "[[25  0]\n",
      " [ 0 34]]\n",
      "===\n",
      "Evaluating SVM\n",
      "Cross-validated Fbeta Score: 1.0\n",
      "Cross-validated ROC AUC Score: 1.0\n",
      "Fbeta Score on Test Set: 1.0\n",
      "[[25  0]\n",
      " [ 0 34]]\n",
      "===\n",
      "Evaluating KNN\n",
      "Cross-validated Fbeta Score: 0.9826107465347972\n",
      "Cross-validated ROC AUC Score: 0.9990476190476191\n",
      "Fbeta Score on Test Set: 0.9770114942528735\n",
      "[[24  1]\n",
      " [ 0 34]]\n",
      "===\n",
      "Evaluating Multinomial Naive Bayes\n",
      "Cross-validated Fbeta Score: 0.7902765241166901\n",
      "Cross-validated ROC AUC Score: 0.8941190476190476\n",
      "Fbeta Score on Test Set: 0.8544303797468353\n",
      "[[21  4]\n",
      " [ 7 27]]\n",
      "===\n",
      "Evaluating Logistic Regression\n",
      "Cross-validated Fbeta Score: 1.0\n",
      "Cross-validated ROC AUC Score: 1.0\n",
      "Fbeta Score on Test Set: 1.0\n",
      "[[25  0]\n",
      " [ 0 34]]\n",
      "===\n",
      "Evaluating Linear Discriminant Analysis\n",
      "Cross-validated Fbeta Score: 0.9042546068883348\n",
      "Cross-validated ROC AUC Score: 0.9613095238095237\n",
      "Fbeta Score on Test Set: 0.9939759036144576\n",
      "[[25  0]\n",
      " [ 1 33]]\n",
      "===\n",
      "Evaluating Quadratic Discriminant Analysis\n",
      "Cross-validated Fbeta Score: 0.6592068540193059\n",
      "Cross-validated ROC AUC Score: 0.7341904761904761\n",
      "Fbeta Score on Test Set: 0.949367088607595\n",
      "[[24  1]\n",
      " [ 4 30]]\n",
      "===\n",
      "Evaluating Gradient Boosting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Fbeta Score: 1.0\n",
      "Cross-validated ROC AUC Score: 1.0\n",
      "Fbeta Score on Test Set: 1.0\n",
      "[[25  0]\n",
      " [ 0 34]]\n",
      "===\n",
      "Evaluating AdaBoost\n",
      "Cross-validated Fbeta Score: 1.0\n",
      "Cross-validated ROC AUC Score: 1.0\n",
      "Fbeta Score on Test Set: 1.0\n",
      "[[25  0]\n",
      " [ 0 34]]\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "classifiers = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=1),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=5, random_state=1),\n",
    "    \"SVM\": SVC(kernel='linear', C=1.0, gamma=2),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=10),\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
    "    \"Quadratic Discriminant Analysis\": QuadraticDiscriminantAnalysis(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=0),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=0)\n",
    "}\n",
    "\n",
    "# Define a custom scoring function for fbeta_score\n",
    "scoring = make_scorer(fbeta_score, beta=0.5)\n",
    "\n",
    "# Create an empty list to store pipelines\n",
    "pipelines = []\n",
    "\n",
    "# Iterate through each classifier\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"Evaluating {name}\")\n",
    "    \n",
    "    # Cross-validation with fbeta_score\n",
    "    cv_scores_fbeta = cross_val_score(classifier, X_imputed, Y, cv=10, scoring=scoring)\n",
    "    print(f\"Cross-validated Fbeta Score: {np.mean(cv_scores_fbeta)}\")\n",
    "    \n",
    "    # Cross-validation with ROC AUC score\n",
    "    cv_scores_roc_auc = cross_val_score(classifier, X_imputed, Y, cv=10, scoring='roc_auc')\n",
    "    print(f\"Cross-validated ROC AUC Score: {np.mean(cv_scores_roc_auc)}\")\n",
    "    \n",
    "    # Create a pipeline with the classifier and fit it\n",
    "    classifier_clone = clone(classifier)\n",
    "    pipeline = Pipeline([\n",
    "        ('classifier', classifier_clone)\n",
    "    ])\n",
    "    pipeline.fit(X_imputed, Y)\n",
    "    pipelines.append((name, pipeline))\n",
    "    \n",
    "    # Make predictions on the test data using the pipeline\n",
    "    predictions_test = pipeline.predict(X_test_imputed)\n",
    "    \n",
    "    # Evaluate the predictions using fbeta_score\n",
    "    fbeta = fbeta_score(Y_test, predictions_test, average='binary', beta=0.5)\n",
    "    \n",
    "    print(f\"Fbeta Score on Test Set: {fbeta}\")\n",
    "    \n",
    "    confusion = metrics.confusion_matrix(Y_test, predictions_test)\n",
    "    print(confusion)\n",
    "    print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['autism_model.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "model_filename = 'autism_model.joblib'\n",
    "joblib.dump(classifiers, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danus\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "pipelines = []\n",
    "\n",
    "# Iterate through each classifier and create a pipeline\n",
    "for name, classifier in classifiers.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', imputer),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    pipeline.fit(X_imputed, Y)\n",
    "    pipelines.append((name, pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['autism_detection_pipelines.pkl']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipelines, 'autism_detection_pipelines.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['autism_imputer.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(imputer, 'autism_imputer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
